1 Bert错误地假设了被覆盖词与被覆盖词之间是独立的。并且使预训练和微调时的输入不统一。

2 GPT只能考虑一个单方向的语境。很多时候的下游任务，如自然语言理解，会同时需要前后两个方向的语境信息。

3  XLNet通过改变字词排列位置的方法将双向的语句排列到单向。在具体实现时，通过对注意力机制的掩膜的作用之一就是进行改变来达到改变排列的目的。

4 基于XLNet，设计出命名实体识别模型，在CoNLL-2003数据集上训练，最终正确分类的标签数：46478，标签总数：46836.0，准确率：99.24%。

​																																--by liuguobing